---
aliases: [palabras del presidente]
title: 'Las palabras del presidente'
thumbnail: ""
authors: [elina]
date: '2023-04-30'
tags: [Ciencias sociales computacionales]
codefolding_show: hide
codefolding_nobutton: true
categories:
  - R
  - Minería de texto
summary: 2023-05-04 / Recuperación y análisis textual de declaraciones presidenciales (2020-2023)
image:
  caption: ''
  focal_point: ''
  output:
  blogdown::html_page:
    toc: yes
    number_sections: true
    toc_depth: 4
featuredImage: ""
---

<script src="{{< blogdown/postref >}}index_files/kePrint/kePrint.js"></script>
<link href="{{< blogdown/postref >}}index_files/lightable/lightable.css" rel="stylesheet" />


<hr />
<p>Las palabras son un campo de disputa, están situadas, se conectan con un pasado y construyen realidad. En esta entrada me interesa ensayar algunas prácticas de recuperación de discursos políticos, en particular, declaraciones del presidente de la República Lacalle Pou y posterior análisis con técnicas de minería de texto usando R.</p>
<p>Intentaremos responder lo siguiente: ¿Cuales son las palabras que son más mencionadas por el presidente en sus declaraciones? ¿y las frases? ¿esto cambia en el tiempo? ¿qué temas se desprenden del análisis de las palabras? ¿cómo se asocian entre sí?</p>
<hr />
<p><br></p>
<p>El ensayo se estructura en dos partes fundamentales: (1) recuperación y (2) procesamiento con visualizaciones asociadas. Para asegurar la replicabilidad del análisis se presenta el código utilizado (desplegable) y se disponibilizan en mi <a href="https://github.com/elinagomez/repo/tree/master/content/blog/2023-05-4-palabras-presidente/Datos">repositorio de GitHub</a> las bases utilizadas para reproducir o ampliar los procesamientos.</p>
<hr />
<p><strong>1. Recuperación:</strong> scraping de declaraciones del presidente en YouTube</p>
<p>El primer desafío para cumplir con los objetivos que me había planteado es recuperar las declaraciones de Lacalle Pou en formato textual y procesable. Para ello, desde algún tiempo la plataforma YouTube permite descargar los subtítulos de los videos para diferentes idiomas por lo que esa podía ser una vía para conseguir esos discursos o declaraciones.</p>
<p>Para focalizar y construir un corpus de datos textuales exhaustivo de las declaraciones del presidente tenía algunas posibilidades. En primer lugar, lograr recopilar todas las url de las declaraciones haciendo una búsqueda simple en YouTube (por ej.: declaraciones+“lacalle pou”) y copiándolas a una hoja de cálculo, pero me llevaría mucho tiempo por lo que opté por intentar automatizar la descargas de url con la librería <a href="https://github.com/tidyverse/rvest">rvest</a> de tidyverse en múltiples páginas de resultado (tal como hice y está documentado en <a href="https://www.elinagomez.com/blog/2020-09-26-parlamento-genero/">este</a> posteo de scraping parlamentario). Esta solución, aunque viable, no me permitía recuperar fácilmente alguna metadata que me interesaba para el análisis (fecha, resumen, canal, etc.) por lo que finalmente utilicé una herramienta de pago que se llama <a href="https://apify.com/">Apify</a> que te da un crédito gratis suficiente para hacer varias búsquedas o scrapeos en diferentes plataformas de redes sociales (YouTube, Facebook, Instagram, Twitter) que puede ser interesante para usar como fuente en investigaciones sociales.</p>
<p>Esta herramienta permite descargar las url (y alguna metadata) para luego poder ejecutar las transcripciones desde R, por lo que con una búsqueda específica (declaraciones+“lacalle pou”) en 2 horas y 13 minutos me recuperó 575 resultados en formato tabulado que coincidían. Al analizar la base veo que es bastante precisa pero hay muchas declaraciones repetidas ya que existe una declaración oficial y los diferentes medios de comunicación la <em>“levantan”</em> y suben en su propio canal de YouTube por lo que también existían intervenciones de periodistas que me <em>“ensuciaban”</em> el análisis. A partir de esta constatación, opté por quedarme con las declaraciones oficiales desde la asunción del presidente (01/03/2020), subidas al canal oficial de YouTube de <a href="https://www.youtube.com/@ComunicacionPresidencial">Presidencia de la República</a>. Luego de hacer ese filtro inicial, me quedo con 205 enlaces y corroboro que todos los videos sean del presidente, a partir de lo cual descarto tres casos que eran de otras personas y no relevantes para mi análisis, me quedo con 202 url a videos.</p>
<p>Para realizar la descarga de los subtitulos de los videos uso la librería <a href="https://github.com/jooyoungseo/youtubecaption">youtubecaption</a> que trabaja sobre la librería de Python <a href="https://pypi.org/project/youtube-transcript-api/">youtube-transcript-api</a> y que que recupera la transcripción de forma tabulada y ordenada para cada secuencia del video, por lo que luego es necesario agrupar por el identificador y recuperar la metadata original. Hago este proceso iterando sobre mi vector de url con <a href="https://purrr.tidyverse.org/">purrr</a> y la función <em>possibly()</em> para saltar errores.</p>
<pre class="r"><code>remotes::install_github(&quot;jooyoungseo/youtubecaption&quot;) #instalo
library(youtubecaption) #cargo librerías
library(dplyr)

x &lt;- base$url ##vector de url
declara &lt;- purrr::map(x,purrr::possibly(~get_caption(.x,language = &quot;es&quot;),
otherwise = NULL))%&gt;%
bind_rows() #itero, agrupo y uno lista</code></pre>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-2">Table 1: </span>Muestra de transcripciones de videos de YouTube con youtubecaption
</caption>
<thead>
<tr>
<th style="text-align:right;">
segment_id
</th>
<th style="text-align:left;">
text
</th>
<th style="text-align:right;">
start
</th>
<th style="text-align:right;">
duration
</th>
<th style="text-align:left;">
vid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
reclamando una pérdida según salarial es
</td>
<td style="text-align:right;">
3.020
</td>
<td style="text-align:right;">
5.770
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
un tema que estamos tratando el artículo
</td>
<td style="text-align:right;">
6.930
</td>
<td style="text-align:right;">
5.629
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
4 del presupuesto se está hablando con
</td>
<td style="text-align:right;">
8.790
</td>
<td style="text-align:right;">
6.890
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
economía se estaba hablando con
</td>
<td style="text-align:right;">
12.559
</td>
<td style="text-align:right;">
5.351
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
presidencia de la república con delegado
</td>
<td style="text-align:right;">
15.680
</td>
<td style="text-align:right;">
6.340
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
de presidencia se está hablando con los
</td>
<td style="text-align:right;">
17.910
</td>
<td style="text-align:right;">
6.450
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Luego, agrupo por identificador y pego todas las transcripciones de cada secuencia de los videos, y recupero la metadata inicial para armar mi base final. Cabe aclarar que hay 17 casos que no se recuperaron los subtítulos porque eran imágenes del presidente sin audio.</p>
<pre class="r"><code>declara_final = declara %&gt;% 
  group_by(vid) %&gt;% 
  mutate(declara = paste0(text, collapse = &quot; &quot;)) %&gt;% #pego declaraciones por identificador
  distinct(vid,declara)%&gt;%
  left_join(base,by=c(&quot;vid&quot;=&quot;id&quot;))%&gt;% ##le pego la metadata
  mutate(anio=substr(date,1,4)) #armo variable de año</code></pre>
<hr />
<p><strong>2. Análisis de los datos textuales</strong></p>
<p>Como punto de partida para el análisis, luego de hacer la recuperación del apartado anterior, cuento con un corpus de datos con 185 declaraciones presidenciales transcriptas que comprenden el período 2020 a 2023<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>En términos de visualizaciones de los videos en YouTube, considerando todo el universo, se desprende que los tres de mayor relevancia fueron las declaraciones en el marco de la cumbre de la Celac (2023) y la Cumbre de Presidentes del Mercosur (2022), que tienen un carácter internacional y por lo tanto mayor impacto, y en tercer lugar, declaraciones vinculadas al caso de corrupción de Astesiano en noviembre de 2022.</p>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-5">Table 2: </span>Frecuencia de declaraciones recuperadas por año
</caption>
<thead>
<tr>
<th style="text-align:left;">
vid
</th>
<th style="text-align:left;">
title
</th>
<th style="text-align:left;">
text
</th>
<th style="text-align:left;">
date
</th>
<th style="text-align:right;">
viewCount
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
-Is2R6ubWOc
</td>
<td style="text-align:left;">
Palabras del presidente Lacalle Pou en la cumbre de la Celac
</td>
<td style="text-align:left;">
En el marco de la VII Cumbre de Estados Latinoamericanos y Caribeños (Celac), que se realiza en la ciudad de Buenos Aires, Argentina, se expresó el presidente de la República, Luis Lacalle Pou
</td>
<td style="text-align:left;">
2023-01-24
</td>
<td style="text-align:right;">
443091
</td>
</tr>
<tr>
<td style="text-align:left;">
iyOWq8yPrKQ
</td>
<td style="text-align:left;">
Palabras del presidente de Uruguay, Luis Lacalle Pou
</td>
<td style="text-align:left;">
Este martes 6, en Montevideo, el mandatario de Uruguay, Luis Lacalle Pou, abrió la Cumbre de Presidentes de los Estados Partes del Mercosur y Estados Asociados.
</td>
<td style="text-align:left;">
2022-12-06
</td>
<td style="text-align:right;">
202972
</td>
</tr>
<tr>
<td style="text-align:left;">
ZMIH-HQMM2I
</td>
<td style="text-align:left;">
Declaraciones del presidente de la República, Luis Lacalle Pou
</td>
<td style="text-align:left;">
Tras participar en una actividad en Maltería Oriental SA, el presidente de la República, Luis Lacalle Pou, realizó declaraciones a la prensa.
</td>
<td style="text-align:left;">
2022-11-30
</td>
<td style="text-align:right;">
90095
</td>
</tr>
</tbody>
</table>
<p>Me interesa explorar, como primer acercamiento, cuáles son las palabras que menciona el presidente en mayor medida pero quiero poder analizarlas distinguiendo entre sus categorías gramaticales. Para eso, necesito alguna herramienta que haga <em>anotaciones</em> e identifique qué categoría corresponde a cada palabra que forma parte de mi corpus, y para ello voy a utilizar un modelo pre-entrenado para español que logra hacer ese etiquetado. Hay algunos paquetes de R para dicho fin (ej. <a href="https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html">spacyr</a>) en base a la librería de Python <em>spacy</em>, pero en este caso vamos a usar <a href="https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html">udpipe</a> que definirá qué etiqueta (POS-tags) corresponde a cada palabra según <a href="https://universaldependencies.org/u/pos/">categorías universales</a>.</p>
<p>Si distinguimos entre categorías, como se ve en el gráfico, se observa que los adjetivos que predominan son de carácter positivo ( <em>importante</em>, <em>bueno</em>, <em>mejor</em>, etc.), mientras que los países que más se mencionan son <em>brasil</em>, <em>argentina</em> y <em>paraguay</em> respectivamente y el verbo más mencionado es <em>tener</em>. Aunque no entra en las categorías gramaticales que estamos viendo ya que es un adverbio, la palabra <em>obviamente</em> es una muletilla del presidente, la menciona 506 veces, es decir, más de 2.5 veces por declaración.</p>
<pre class="r"><code>install.packages(&quot;udpipe&quot;)
library(udpipe)
#Descargo el modelo en una ruta determinada
modeloES &lt;- udpipe_download_model(language = &quot;ruta/spanish-gsd&quot;)
#Cargo el modelo desde esa ruta
modeloES &lt;- udpipe_load_model(language = &quot;spanish-gsd&quot;)
anotado &lt;- udpipe_annotate(modeloES, x = declara_final$declara,
doc_id = declara_final$vid)%&gt;%
as.data.frame()

##Gráfico según categoría gramatical

anotado %&gt;%
  filter(upos %in% c(&quot;NOUN&quot;,&quot;PROPN&quot;,&quot;ADJ&quot;,&quot;VERB&quot;))%&gt;%
  mutate(upos = recode(upos, NOUN = &#39;Sustantivos&#39;, PROPN = &#39;Nombres propios&#39;, 
  ADJ =  &#39;Adjetivos&#39;,VERB =  &#39;Verbos&#39; ))%&gt;%
  filter(upos%in%quanteda::stopwords(language = &quot;es&quot;)==F)%&gt;%
  group_by(lemma,upos) %&gt;%
  summarize(n=n())%&gt;%
  group_by(upos)%&gt;%
  top_n(15, abs(n)) %&gt;%
  ggplot(aes(reorder(lemma, n), n,fill=upos)) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ upos, scales = &quot;free&quot;) +
  xlab(&quot;&quot;) +
  ylab(&quot;&quot;) +
  theme_bw() +
  coord_flip()</code></pre>
<iframe src="unnamed-chunk-7-1.png" width="700" height="500">
</iframe>
<p><br></p>
<p>Luego, voy a focalizar el análisis en algunas palabras específicas y observar la evolución entre los años de los cuales tenemos declaraciones, en un mapa de calor. Para ello, calculo el peso relativo de cada palabra en el total de palabras mencionadas por el presidente ese año (ya que tenemos menos declaraciones de 2023) y comparo. Tiene sentido que en los primeros años (2020 y 2021) tenga más peso las palabras <em>pandemia</em> y <em>salud</em>, mientras que social y economía empiezan a tener más relevancia desde 2022. Las palabras corrupción, sindicato e ideología tienen un peso relativo menor con respecto a las demás.</p>
<pre class="r"><code>anotado %&gt;%
  left_join(.,declara_final%&gt;%select(anio),by=c(&quot;doc_id&quot;=&quot;vid&quot;))%&gt;%
  filter(upos%in%quanteda::stopwords(language = &quot;es&quot;)==F)%&gt;%
  group_by(anio,lemma) %&gt;%
  summarise(n = n()) %&gt;%
  mutate(prop = round((n / sum(n)*100),3))%&gt;%
  filter(lemma %in% c(&quot;libertad&quot;,&quot;ideología&quot;,&quot;pandemia&quot;,&quot;economía&quot;,&quot;social&quot;,&quot;salud&quot;,
                      &quot;coalición&quot;,&quot;corrupción&quot;,&quot;argentina&quot;,&quot;brasil&quot;,&quot;china&quot;,&quot;reforma&quot;,
                      &quot;sindicato&quot;))%&gt;% ##palabras que selecciono
group_by(anio)%&gt;%
ggplot(aes(x = anio , y = reorder(lemma, prop))) +
  geom_tile(aes(fill = prop), color = &quot;white&quot;, size = 1) +
  scale_fill_gradient(high = &quot;#C77CFF&quot;,low = &quot;white&quot;) +
  theme_bw() +
  theme(axis.ticks = element_blank(),
        panel.background = element_blank(),
        axis.title = element_blank())</code></pre>
<iframe src="unnamed-chunk-9-1.png" width="700" height="500">
</iframe>
<p><br></p>
<p>Si quiero ir un poco más allá de las palabras y extraer los términos claves que menciona Lacalle Pou en sus declaraciones, me quedo únicamente con los sustantivos, adjetivos y nombres propios, y utilizo la función <em>keywords_rake</em> del mismo paquete <a href="https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html">udpipe</a> que construye una métrica ( <em>ranke</em>) que surge de combinar la frecuencia de aparición de una palabra y la co-ocurrencia con otras en un documento determinado, en este caso, en cada declaración. Como criterio general, voy a considerar términos claves de hasta tres palabras y opto por quedarme con el <em>lema</em> de cada una, el cual corresponde a la forma común (ej. público) a todas las formas que puede adquirir una palabra (públicos, pública, pública, etc.) y así evitar repeticiones.</p>
<p>Las palabras claves nos dan una idea de las temáticas que trata el presidente en sus declaraciones públicas y su jerarquización en términos de presencia mayoritaria de ciertos términos. Lo sanitario aparece en primer lugar, vinculado a la pandemia, seguido por la ley de urgente consideración, así como tópicos sobre impuestos, aranceles y comercio exterior.</p>
<pre class="r"><code>udpipe::keywords_rake(x = anotado %&gt;%filter(lemma %in%quanteda::stopwords(language = &quot;es&quot;)==F),term = &quot;lemma&quot;, group = &quot;doc_id&quot;, 
relevant = anotado %&gt;%filter(lemma %in%quanteda::stopwords(language = &quot;es&quot;)==F)%&gt;% pull(upos) %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;,&quot;PROPN&quot;),
ngram_max = 3)%&gt;% # Sólo sustantivos, adjetivos y nombres propios
filter(freq&gt;4)%&gt;%
  arrange(rake) %&gt;%
  top_n(30, abs(rake)) %&gt;%
  ggplot(aes(reorder(keyword, rake), rake )) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE,fill= &quot;#00bfc4&quot;) +
   scale_y_continuous(limits = c(0,NA)) +
  xlab(&quot;&quot;) +
  ylab(&quot;&quot;) +
  theme_bw() +
  coord_flip()</code></pre>
<iframe src="unnamed-chunk-11-1.png" width="700" height="500">
</iframe>
<p><br></p>
<p>Otra forma de acercarnos a lo que se menciona en las declaraciones, tiene que ver con la identificación de <em>frases</em> o <em>sintagmas nominales</em>, es decir, que se construyen a partir de un <em>sustantivo</em> como núcleo de la frase. Esto lo puedo identificar utilizando <a href="https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular">expresiones regulares</a> que me permiten extraer frases a partir de un patrón que combina las formas gramaticales previamente etiquetadas. Con ese fin, uso la función <em>keywords_phrases</em>.</p>
<pre class="r"><code>library(udpipe)
anotado$phrase_tag &lt;- udpipe::as_phrasemachine(anotado$upos, type = &quot;upos&quot;)
anotado$phrase_tag[anotado$upos==&quot;PRON&quot;] &lt;- &quot;O&quot;
anotado$phrase_tag[anotado$upos==&quot;NUM&quot;] &lt;- &quot;O&quot;
udpipe::keywords_phrases(x = anotado$phrase_tag, term = tolower(anotado$token),
pattern = &quot;(A|N)*N(P+D*(A|N)*N)*&quot;,
is_regex = TRUE, detailed = FALSE)%&gt;%
filter(ngram &gt; 1 &amp; freq &gt; 3)%&gt;%
arrange(freq) %&gt;%
top_n(30, abs(freq)) %&gt;%
ggplot(aes(reorder(keyword, freq), freq ,fill=&quot;#00BFC4&quot;)) +
geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
xlab(&quot;&quot;) +
ylab(&quot;&quot;) +
theme_bw() +
coord_flip()</code></pre>
<iframe src="unnamed-chunk-13-1.png" width="700" height="500">
</iframe>
<p><br></p>
<p>También podríamos indagar un poco más el contexto de aparición de ciertas palabras o expresionescon la función <em>kwic</em> ( <em>keyword in context</em>) del paquete <a href="http://quanteda.io/">quanteda</a>. Esta técnica nos permite focalizar en una palabra o frase y analizar el contexto en que se menciona en un determinado texto, definiendo una <em>ventana</em> (cantidad de palabras anteriores y posteriores), pudiendo también constituir un nuevo corpus de datos textuales procesable.</p>
<pre class="r"><code>library(quanteda)

kwic=quanteda::kwic(declara_final$declara,phrase(&quot;sindi*&quot;),window = 15)%&gt;% as.data.frame() ##el * hace que busque todas la variantes con una determinada raíz</code></pre>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-15">Table 3: </span>Contexto de palabra clave
</caption>
<thead>
<tr>
<th style="text-align:right;">
from
</th>
<th style="text-align:right;">
to
</th>
<th style="text-align:left;">
pre
</th>
<th style="text-align:left;">
keyword
</th>
<th style="text-align:left;">
post
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
351
</td>
<td style="text-align:right;">
351
</td>
<td style="text-align:left;">
que empezaron las clases presidente comenzó con un par de actividades por parte de los
</td>
<td style="text-align:left;">
sindicatos
</td>
<td style="text-align:left;">
de Montevideo sobre todo de secundaria de usted teme que esta situación de los sindicatos
</td>
</tr>
<tr>
<td style="text-align:right;">
366
</td>
<td style="text-align:right;">
366
</td>
<td style="text-align:left;">
sindicatos de Montevideo sobre todo de secundaria de usted teme que esta situación de los
</td>
<td style="text-align:left;">
sindicatos
</td>
<td style="text-align:left;">
frenen algo en la transformación educativa y la postura que están teniendo muy crítica temor
</td>
</tr>
<tr>
<td style="text-align:right;">
1189
</td>
<td style="text-align:right;">
1189
</td>
<td style="text-align:left;">
desalojar como establece la ley Así que el juego de la Democracia es este los
</td>
<td style="text-align:left;">
sindicatos
</td>
<td style="text-align:left;">
tienen la defensa no sé si todos los docentes o todo el personal no docente
</td>
</tr>
<tr>
<td style="text-align:right;">
1518
</td>
<td style="text-align:right;">
1518
</td>
<td style="text-align:left;">
trabajo en conjunto el Ministerio de economía la anep el Ministerio de trabajo y los
</td>
<td style="text-align:left;">
sindicatos
</td>
<td style="text-align:left;">
se inició la recuperación salarial para docentes y funcionarios se terminaba en la desigualdades históricas
</td>
</tr>
<tr>
<td style="text-align:right;">
1768
</td>
<td style="text-align:right;">
1768
</td>
<td style="text-align:left;">
reglamentación del artículo 57 de la Constitución que es la personería jurídica de las organizaciones
</td>
<td style="text-align:left;">
sindicales
</td>
<td style="text-align:left;">
yo creo que eso va a generar un funcionamiento mucho más garantista el año pasado
</td>
</tr>
</tbody>
</table>
<p>Es posible analizar, por ejemplo, qué adjetivos o verbos menciona el presidente en torno a la palabras <em>“sindicato”</em> (podrían también ser términos multi-palabra como <em>“reforma de la seguridad social”</em> o <em>“transformación educativa”</em>).</p>
<pre class="r"><code>library(ggplot2)
b=quanteda::kwic(declara_final$declara,phrase(&quot;sindi*&quot;),window = 30)
b$texto=paste(b$pre,b$post)

sindi &lt;- udpipe_annotate(modeloES,x = b$texto,
                           doc_id = b$docname)
sindi &lt;- as.data.frame(sindi)
sindi %&gt;%
  filter(upos %in% c(&quot;ADJ&quot;,&quot;VERB&quot;))%&gt;%
  mutate(upos = recode(upos, ADJ =  &#39;Adjetivos&#39;,VERB =  &#39;Verbos&#39; ))%&gt;%
  filter(upos%in%quanteda::stopwords(language = &quot;es&quot;)==F)%&gt;%
  group_by(lemma,upos) %&gt;%
  summarize(n=n())%&gt;%
  group_by(upos)%&gt;%
  top_n(12, abs(n)) %&gt;%
  ggplot(aes(reorder(lemma, n), n,fill=upos)) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ upos, scales = &quot;free&quot;,ncol = 2) +
  xlab(&quot;&quot;) +
  ylab(&quot;&quot;) +
  theme_bw() +
  coord_flip()</code></pre>
<iframe src="unnamed-chunk-17-1.png" width="700" height="500">
</iframe>
<p><br></p>
<p>Otro ejercicio que podría hacer tiene que ver con buscar la correlación, con la función <em>textstat_simil</em> de <em>quanteda</em>, de una palabras con otras en términos de co-ocurrencia en cada una de las declaraciones públicas de Lacalle Pou. Como se observa, la palabra <em>libertad</em> en sus 144 menciones se asocia principalmente a nivel discursivo con la religión, las creencias, así como con otros términos que la complementan (suprema, ejercicio, demostración).</p>
<pre class="r"><code>library(quanteda)

quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
remove_numbers = T),tolower=TRUE,verbose = FALSE) %&gt;%
  quanteda::dfm_remove(pattern = c(quanteda::stopwords(&quot;spanish&quot;)),
min_nchar=3)%&gt;%
quanteda.textstats::textstat_simil(selection = &quot;libertad&quot;,
method = &quot;correlation&quot;,margin = &quot;features&quot;)%&gt;%
as.data.frame()%&gt;%
arrange(-correlation)%&gt;%
head(15)</code></pre>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-19">Table 4: </span>Correlación entre palabras
</caption>
<thead>
<tr>
<th style="text-align:left;">
feature1
</th>
<th style="text-align:left;">
feature2
</th>
<th style="text-align:right;">
correlation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
demostraron
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5253191
</td>
</tr>
<tr>
<td style="text-align:left;">
religiosas
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5244921
</td>
</tr>
<tr>
<td style="text-align:left;">
aseguran
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5244921
</td>
</tr>
<tr>
<td style="text-align:left;">
creencias
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5244921
</td>
</tr>
<tr>
<td style="text-align:left;">
apelar
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5192013
</td>
</tr>
<tr>
<td style="text-align:left;">
suprema
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5141596
</td>
</tr>
<tr>
<td style="text-align:left;">
país
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5117357
</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Por último, me interesa indagar sobre qué peso tienen los principales temas a los que se refiere Lacalle Pou utilizando diccionarios temáticos que me van a dar información al respecto <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. Con este objetivo, armo un diccionario considerando palabras y términos asociados a 6 dimensiones (social, economía, salud, seguridad, vivienda y educación) y observo su peso general y evolución por año <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
<p>Se observa, como es de esperar, una predominancia de los temas vinculados a salud como consecuencia de la pandemia en los primeros años (2020-2021), mientras que la economía y la educación, esta sobre todo en el último año, cobra relevancia en sus declaraciones.</p>
<pre class="r"><code>dfm=quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
                         remove_numbers = T),tolower=TRUE,verbose = FALSE) %&gt;%
  quanteda::dfm_remove(pattern = c(quanteda::stopwords(&quot;spanish&quot;)),
                       min_nchar=3)

docnames(dfm)= declara_final$vid
midic &lt;- dictionary(list(Social = c(&quot;soci*&quot;,&quot;politica social&quot;,&quot;politicas sociales&quot;, &quot;plan social&quot;,
                                    &quot;planes sociales&quot;,&quot;pobreza&quot;),
                         Economia = c(&quot;econo*&quot;,&quot;empleo&quot;, &quot;desempleo&quot;, &quot;crisis&quot;,&quot;fiscal&quot;,&quot;dolar*&quot;,&quot;inflación&quot;,
                                      &quot;moned*&quot;,&quot;diner*&quot;,&quot;deuda&quot;,&quot;déficit&quot;),
                         Seguridad=c(&quot;seguridad&quot;,&quot;robo&quot;,&quot;delincuente&quot;,&quot;inseguridad&quot;,&quot;homicidio&quot;,
                                     &quot;rapiña&quot;,&quot;delito&quot;),
                         Vivienda = c(&quot;vivienda&quot;,&quot;habitacional&quot;,&quot;asentamiento*&quot;),
                         Educacion = c(&quot;educ*&quot;,&quot;transformación educativa&quot;,&quot;escuela&quot;,&quot;liceo&quot;,&quot;utu&quot;,
                                       &quot;universi*&quot;),
                         Salud = c(&quot;salu*&quot;,&quot;sanitari*&quot;,&quot;pandemia&quot;,&quot;vacuna*&quot;,&quot;salud pública&quot;,
                                       &quot;mutualista&quot;,&quot;ministerio de salud&quot;)))

midic_result&lt;-dfm_lookup(dfm,dictionary=midic)
midic_result=convert(midic_result, to = &quot;data.frame&quot;) %&gt;%
left_join(.,declara_final%&gt;% dplyr::select(anio),by=c(&quot;doc_id&quot;=&quot;vid&quot;))
  
midic_result %&gt;%
  dplyr::select(-doc_id)%&gt;%
  group_by(anio)%&gt;%
  summarise(across(where(is.numeric), ~ sum(.x, na.rm = TRUE)))%&gt;%
  pivot_longer(-anio)%&gt;%
  group_by(anio)%&gt;%
  mutate(prop = round((value / sum(value)*100),1))%&gt;%
  ggplot(aes(x = anio , y = prop,fill=name)) +
  geom_bar(position=&quot;fill&quot;, stat=&quot;identity&quot;)+
  theme_bw() + scale_y_continuous(labels = scales::percent_format(accuracy = 1))+
  theme(axis.ticks = element_blank(),
        panel.background = element_blank(),
        axis.title = element_blank(),
        legend.title = element_blank())</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<hr />
<p>Hasta aquí algunos de los ensayos de recuperación y análisis de los datos textuales. Las técnicas que explora el posteo resultan útiles para realizar un análisis inicial y rápido de los principales tópicos, términos y sus combinaciones, agrupamientos de palabras, con visualizaciones ilustrativas que ayudan a interpretar los resultados. Pueden complementarse o combinarse con un análisis e interpretación en profundidad de los datos textuales, por ejemplo, de forma asistida con la herramienta libre de análisis cualitativo <a href="https://rqda.r-forge.r-project.org/">RQDA</a>, lo cual puede retroalimentar el análisis, a partir de identificar categorías teóricas de forma sistemática que se integren y oficien de variables de corte para las técnicas y procesamientos presentados.</p>
<hr />
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Para el año 2020 se recuperaron sólo 5 casos por lo que, cuando existan cruces por año, opto presentarlos en conjunto con los de 2021. Esto puede deberse a que se registró una baja en las declaraciones en situación de pandemia, existiendo una comunicación más centralizada, y en algunos casos, no aparecía la palabra <em>declaraciones</em> por lo que el filtro sub registró esos casos al automatizar la búsqueda.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>En español hay poco o nulo desarrollo de diccionarios de temas[^2] y es una linea importante a trabajar. Existe un proyecto interesante denominado <a href="https://www.comparativeagendas.net/">Comparative Agendas Project</a> que disponibiliza un diccionario de al momento de 21 temas y 220 subtemas para inglés y holandés.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>El utilizado es un diccionario preliminar que podría ampliarse y mejorarse para obtener mejores resultados e interpretaciones analíticas.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
