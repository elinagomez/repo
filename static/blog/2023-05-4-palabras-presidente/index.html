---
aliases: [palabras del presidente]
title: 'Las palabras del presidente'
thumbnail: ""
authors: [elina]
date: '2023-04-30'
tags: [Ciencias sociales computacionales]
codefolding_show: hide
codefolding_nobutton: true
categories:
  - R
  - Minería de texto
summary: 2023-05-04 / Recuperación y análisis textual de declaraciones presidenciales (2020-2023)
image:
  caption: ''
  focal_point: ''
  output:
  blogdown::html_page:
    toc: yes
    number_sections: true
    toc_depth: 4
featuredImage: ""
---

<script src="{{< blogdown/postref >}}index_files/kePrint/kePrint.js"></script>
<link href="{{< blogdown/postref >}}index_files/lightable/lightable.css" rel="stylesheet" />


<hr />
<p>Las palabras son un campo de disputa, están situadas, se conectan con un pasado y construyen realidad. En esa entrada me interesa ensayar algunas prácticas de recuperación de discursos políticos, en particular, declaraciones del presidente de la República Lacalle Pou y posterior análisis con técnicas de minería de texto usando R.</p>
<p>Intentaremos responder lo siguiente: ¿Cuales son las palabras que son más mencionadas por el presidente en sus declaraciones? ¿y las frases? ¿esto cambia en el tiempo? ¿qué temas se desprenden del análisis de las palabras? ¿cómo se asocian entre sí?</p>
<hr />
<p><br></p>
<p>El ensayo se estructura en dos partes fundamentales: (1) recuperación y (2) procesamiento con visualizaciones asociadas. Para asegurar la replicabilidad del análisis se presenta el código utilizado (desplegable) y se disponibilizan en mi <a href="https://github.com/elinagomez/repo/tree/master/content/blog/2023-05-4-palabras-presidente/Datos">repositorio de GitHub</a> las bases utilizadas para reproducir o ampliar los procesamientos.</p>
<hr />
<p><strong>1. Recuperación:</strong> scraping de declaraciones del presidente en YouTube</p>
<p>El primer desafío para cumplir con los objetivos que me había planteado es recuperar las declaraciones de Lacalle Pou en formato textual y procesable. Para ello, desde algún tiempo la plataforma YouTube permite descargar los subtítulos de los videos para diferentes idiomas por lo que esa podía ser una vía para conseguir esos discursos o declaraciones.</p>
<p>Para focalizar y construir un corpus de datos textuales exhaustivo de las declaraciones del presidente tenía algunas posibilidades. En primer lugar, lograr recopilar todas las url de las declaraciones haciendo una búsqueda simple en YouTube (por ej.: declaraciones+“lacalle pou”) y copiándolas a una hoja de cálculo, pero me llevaría mucho tiempo por lo que opté por intentar automatizar la descargas de url con la librería <a href="https://github.com/tidyverse/rvest">rvest</a> de tidyverse en múltiples páginas de resultado (tal como hice y está documentado en <a href="https://www.elinagomez.com/blog/2020-09-26-parlamento-genero/">este</a> posteo de scraping parlamentario). Esta solución, aunque viable, no me permitía recuperar fácilmente alguna metadata que me interesaba para el análisis (fecha, resumen, canal, etc.) por lo que finalmente utilicé una herramienta de pago que se llama <a href="https://apify.com/">Apify</a> que te da un crédito gratis suficiente para hacer varias búsquedas o scrapeos en diferentes plataformas de redes sociales (YouTube, Facebook, Instagram, Twitter) que puede ser interesante para usar como fuente en investigaciones sociales.</p>
<p>Esta herramienta permite descargar las url (y alguna metadata) para luego poder ejecutar las transcripciones desde R, por lo que con una búsqueda específica (declaraciones+“lacalle pou”) en 2 horas y 13 minutos me recuperó 575 resultados en formato tabulado que coincidían. Al analizar la base veo que es bastante precisa pero hay muchas declaraciones repetidas ya que existe una declaración oficial y los diferentes medios de comunicación la <em>“levantan”</em> y suben en su propio canal de YouTube por lo que también existían intervenciones de periodistas que me <em>“ensuciaban”</em> el análisis. A partir de esta constatación, opté por quedarme con las declaraciones oficiales desde la asunción del presidente (01/03/2020), subidas al canal oficial de YouTube de <a href="https://www.youtube.com/@ComunicacionPresidencial">Presidencia de la República</a>. Luego de hacer ese filtro inicial, me quedo con 205 enlaces y corroboro que todos los videos sean del presidente, a partir de lo cual descarto tres casos que eran de otras personas y no relevantes para mi análisis, me quedo con 202 url a videos.</p>
<p>Para realizar la descarga de los subtitulos de los videos uso la librería <a href="https://github.com/jooyoungseo/youtubecaption">youtubecaption</a> que trabaja sobre la librería de Python <a href="https://pypi.org/project/youtube-transcript-api/">youtube-transcript-api</a> y que que recupera la transcripción de forma tabulada y ordenada para cada secuencia del video, por lo que luego es necesario agrupar por el identificador y recuperar la metadata original. Hago este proceso iterando sobre mi vector de url con <a href="https://purrr.tidyverse.org/">purrr</a> y la función <em>possibly()</em> para saltar errores.</p>
<pre class="r"><code>remotes::install_github(&quot;jooyoungseo/youtubecaption&quot;) #instalo
library(youtubecaption) #cargo librerías
library(dplyr)

x &lt;- base$url ##vector de url
declara &lt;- purrr::map(x,purrr::possibly(~get_caption(.x,language = &quot;es&quot;),
otherwise = NULL))%&gt;%
bind_rows() #itero, agrupo y uno lista</code></pre>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-2">Table 1: </span>Muestra de transcripciones de videos de YouTube con youtubecaption
</caption>
<thead>
<tr>
<th style="text-align:right;">
segment_id
</th>
<th style="text-align:left;">
text
</th>
<th style="text-align:right;">
start
</th>
<th style="text-align:right;">
duration
</th>
<th style="text-align:left;">
vid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
reclamando una pérdida según salarial es
</td>
<td style="text-align:right;">
3.020
</td>
<td style="text-align:right;">
5.770
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
un tema que estamos tratando el artículo
</td>
<td style="text-align:right;">
6.930
</td>
<td style="text-align:right;">
5.629
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
4 del presupuesto se está hablando con
</td>
<td style="text-align:right;">
8.790
</td>
<td style="text-align:right;">
6.890
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
economía se estaba hablando con
</td>
<td style="text-align:right;">
12.559
</td>
<td style="text-align:right;">
5.351
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
presidencia de la república con delegado
</td>
<td style="text-align:right;">
15.680
</td>
<td style="text-align:right;">
6.340
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
de presidencia se está hablando con los
</td>
<td style="text-align:right;">
17.910
</td>
<td style="text-align:right;">
6.450
</td>
<td style="text-align:left;">
ZG2nJ4NsV_k
</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Luego, agrupo por identificador y pego todas las transcripciones de cada secuencia de los videos, y recupero la metadata inicial para armar mi base final. Cabe aclarar que hay 17 casos que no se recuperaron los subtítulos porque eran imágenes del presidente sin audio.</p>
<pre class="r"><code>declara_final = declara %&gt;% 
  group_by(vid) %&gt;% 
  mutate(declara = paste0(text, collapse = &quot; &quot;)) %&gt;% #pego declaraciones por identificador
  distinct(vid,declara)%&gt;%
  left_join(base,by=c(&quot;vid&quot;=&quot;id&quot;))%&gt;% ##le pego la metadata
  mutate(anio=substr(date,1,4)) #armo variable de año</code></pre>
<hr />
<p><strong>2. Análisis de los datos textuales</strong></p>
<p>Como punto de partida para el análisis, luego de hacer la recuperación del apartado anterior, cuento con un corpus de datos con 185 declaraciones presidenciales transcriptas que comprenden el período 2020 a 2023<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>En términos de visualizaciones de los videos en YouTube, considerando todo el universo, se desprende que los tres de mayor relevancia fueron las declaraciones en el marco de la cumbre de la Celac (2023) y la Cumbre de Presidentes del Mercosur (2022), que tienen un carácter internacional y por lo tanto mayor impacto, y en tercer lugar, declaraciones vinculadas al caso de corrupción de Astesiano en noviembre de 2022.</p>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-5">Table 2: </span>Frecuencia de declaraciones recuperadas por año
</caption>
<thead>
<tr>
<th style="text-align:left;">
vid
</th>
<th style="text-align:left;">
title
</th>
<th style="text-align:left;">
text
</th>
<th style="text-align:left;">
date
</th>
<th style="text-align:right;">
viewCount
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
-Is2R6ubWOc
</td>
<td style="text-align:left;">
Palabras del presidente Lacalle Pou en la cumbre de la Celac
</td>
<td style="text-align:left;">
En el marco de la VII Cumbre de Estados Latinoamericanos y Caribeños (Celac), que se realiza en la ciudad de Buenos Aires, Argentina, se expresó el presidente de la República, Luis Lacalle Pou
</td>
<td style="text-align:left;">
2023-01-24
</td>
<td style="text-align:right;">
443091
</td>
</tr>
<tr>
<td style="text-align:left;">
iyOWq8yPrKQ
</td>
<td style="text-align:left;">
Palabras del presidente de Uruguay, Luis Lacalle Pou
</td>
<td style="text-align:left;">
Este martes 6, en Montevideo, el mandatario de Uruguay, Luis Lacalle Pou, abrió la Cumbre de Presidentes de los Estados Partes del Mercosur y Estados Asociados.
</td>
<td style="text-align:left;">
2022-12-06
</td>
<td style="text-align:right;">
202972
</td>
</tr>
<tr>
<td style="text-align:left;">
ZMIH-HQMM2I
</td>
<td style="text-align:left;">
Declaraciones del presidente de la República, Luis Lacalle Pou
</td>
<td style="text-align:left;">
Tras participar en una actividad en Maltería Oriental SA, el presidente de la República, Luis Lacalle Pou, realizó declaraciones a la prensa.
</td>
<td style="text-align:left;">
2022-11-30
</td>
<td style="text-align:right;">
90095
</td>
</tr>
</tbody>
</table>
<p>Me interesa explorar, como primer acercamiento, cuáles son las palabras que menciona el presidente en mayor medida pero quiero poder analizarlas distinguiendo entre sus categorías gramaticales. Para eso, necesito alguna herramienta que haga <em>anotaciones</em> e identifique qué categoría corresponde a cada palabra que forma parte de mi corpus, y para ello voy a utilizar un modelo pre-entrenado para español que logra hacer ese etiquetado. Hay algunos paquetes de R para dicho fin (ej. <a href="https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html">spacyr</a>) en base a la librería de Python <em>spacy</em>, pero en este caso vamos a usar <a href="https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html">udpipe</a> que definirá qué etiqueta (POS-tags) corresponde a cada palabra según <a href="https://universaldependencies.org/u/pos/">categorías universales</a>.</p>
<p>Si distinguimos entre categorías, como se ve en el gráfico, se observa que los adjetivos que predominan son de carácter positivo ( <em>importante</em>, <em>bueno</em>, <em>mejor</em>, etc.), mientras que los países que más se mencionan son <em>brasil</em>, <em>argentina</em> y <em>paraguay</em> respectivamente y el verbo más mencionado es <em>tener</em>. Aunque no entra en las categorías gramaticales que estamos viendo ya que es un adverbio, la palabra <em>obviamente</em> es una muletilla del presidente, la menciona 506 veces, es decir, más de 2.5 veces por declaración.</p>
<pre class="r"><code>install.packages(&quot;udpipe&quot;)
library(udpipe)
#Descargo el modelo en una ruta determinada
modeloES &lt;- udpipe_download_model(language = &quot;ruta/spanish-gsd&quot;)
#Cargo el modelo desde esa ruta
modeloES &lt;- udpipe_load_model(language = &quot;spanish-gsd&quot;)
anotado &lt;- udpipe_annotate(modeloES, x = declara_final$declara,
doc_id = declara_final$vid)%&gt;%
as.data.frame()

##Gráfico según categoría gramatical

anotado %&gt;%
  filter(upos %in% c(&quot;NOUN&quot;,&quot;PROPN&quot;,&quot;ADJ&quot;,&quot;VERB&quot;))%&gt;%
  mutate(upos = recode(upos, NOUN = &#39;Sustantivos&#39;, PROPN = &#39;Nombres propios&#39;, 
  ADJ =  &#39;Adjetivos&#39;,VERB =  &#39;Verbos&#39; ))%&gt;%
  filter(upos%in%quanteda::stopwords(language = &quot;es&quot;)==F)%&gt;%
  group_by(lemma,upos) %&gt;%
  summarize(n=n())%&gt;%
  group_by(upos)%&gt;%
  top_n(15, abs(n)) %&gt;%
  ggplot(aes(reorder(lemma, n), n,fill=upos)) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ upos, scales = &quot;free&quot;) +
  xlab(&quot;&quot;) +
  ylab(&quot;&quot;) +
  theme_bw() +
  coord_flip()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Luego, voy a focalizar el análisis en algunas palabras específicas y observar la evolución entre los años de los cuales tenemos declaraciones, en un mapa de calor. Para ello, calculo el peso relativo de cada palabra en el total de palabras mencionadas por el presidente ese año (ya que tenemos menos declaraciones de 2023) y comparo. Tiene sentido que en los primeros años (2020 y 2021) tenga más peso las palabras <em>pandemia</em> y <em>salud</em>, mientras que social y economía empiezan a tener más relevancia desde 2022. Las palabras corrupción, sindicato e ideología tienen un peso relativo menor con respecto a las demás.</p>
<pre class="r"><code>anotado %&gt;%
  left_join(.,declara_final%&gt;%select(anio),by=c(&quot;doc_id&quot;=&quot;vid&quot;))%&gt;%
  filter(upos%in%quanteda::stopwords(language = &quot;es&quot;)==F)%&gt;%
  group_by(anio,lemma) %&gt;%
  summarise(n = n()) %&gt;%
  mutate(prop = round((n / sum(n)*100),3))%&gt;%
  filter(lemma %in% c(&quot;libertad&quot;,&quot;ideología&quot;,&quot;pandemia&quot;,&quot;economía&quot;,&quot;social&quot;,&quot;salud&quot;,
                      &quot;coalición&quot;,&quot;corrupción&quot;,&quot;argentina&quot;,&quot;brasil&quot;,&quot;china&quot;,&quot;reforma&quot;,
                      &quot;sindicato&quot;))%&gt;% ##palabras que selecciono
group_by(anio)%&gt;%
ggplot(aes(x = anio , y = reorder(lemma, prop))) +
  geom_tile(aes(fill = prop), color = &quot;white&quot;, size = 1) +
  scale_fill_gradient(high = &quot;#C77CFF&quot;,low = &quot;white&quot;) +
  theme_bw() +
  theme(axis.ticks = element_blank(),
        panel.background = element_blank(),
        axis.title = element_blank())</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Si quiero ir un poco más allá de las palabras y extraer los términos claves que menciona Lacalle Pou en sus declaraciones, me quedo únicamente con los sustantivos, adjetivos y nombres propios, y utilizo la función <em>keywords_rake</em> del mismo paquete <a href="https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html">udpipe</a> que construye una métrica ( <em>ranke</em>) que surge de combinar la frecuencia de aparición de una palabra y la co-ocurrencia con otras en un documento determinado, en este caso, en cada declaración. Como criterio general, voy a considerar términos claves de hasta tres palabras y opto por quedarme con el <em>lema</em> de cada una, el cual corresponde a la forma común (ej. público) a todas las formas que puede adquirir una palabra (públicos, pública, pública, etc.) y así evitar repeticiones.</p>
<p>Las palabras claves nos dan una idea de las temáticas que trata el presidente en sus declaraciones públicas y su jerarquización en términos de presencia mayoritaria de ciertos términos. Lo sanitario aparece en primer lugar, vinculado a la pandemia, seguido por la ley de urgente consideración, así como tópicos sobre impuestos, aranceles y comercio exterior.</p>
<pre class="r"><code>udpipe::keywords_rake(x = anotado %&gt;%filter(lemma %in%quanteda::stopwords(language = &quot;es&quot;)==F),term = &quot;lemma&quot;, group = &quot;doc_id&quot;, 
relevant = anotado %&gt;%filter(lemma %in%quanteda::stopwords(language = &quot;es&quot;)==F)%&gt;% pull(upos) %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;,&quot;PROPN&quot;),
ngram_max = 3)%&gt;% # Sólo sustantivos, adjetivos y nombres propios
filter(freq&gt;4)%&gt;%
  arrange(rake) %&gt;%
  top_n(30, abs(rake)) %&gt;%
  ggplot(aes(reorder(keyword, rake), rake )) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE,fill= &quot;#00bfc4&quot;) +
   scale_y_continuous(limits = c(0,NA)) +
  xlab(&quot;&quot;) +
  ylab(&quot;&quot;) +
  theme_bw() +
  coord_flip()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><br></p>
<p>Otra forma de acercarnos a lo que se menciona en las declaraciones, tiene que ver con la identificación de <em>frases</em> o <em>sintagmas nominales</em>, es decir, que se construyen a partir de un <em>sustantivo</em> como núcleo de la frase. Esto lo puedo identificar utilizando <a href="https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular">expresiones regulares</a> que me permiten extraer frases a partir de un patrón que combina las formas gramaticales previamente etiquetadas. Con ese fin, uso la función <em>keywords_phrases</em>.</p>
<pre class="r"><code>library(udpipe)
anotado$phrase_tag &lt;- udpipe::as_phrasemachine(anotado$upos, type = &quot;upos&quot;)
anotado$phrase_tag[anotado$upos==&quot;PRON&quot;] &lt;- &quot;O&quot;
anotado$phrase_tag[anotado$upos==&quot;NUM&quot;] &lt;- &quot;O&quot;
udpipe::keywords_phrases(x = anotado$phrase_tag, term = tolower(anotado$token),
pattern = &quot;(A|N)*N(P+D*(A|N)*N)*&quot;,
is_regex = TRUE, detailed = FALSE)%&gt;%
filter(ngram &gt; 1 &amp; freq &gt; 3)%&gt;%
arrange(freq) %&gt;%
top_n(30, abs(freq)) %&gt;%
ggplot(aes(reorder(keyword, freq), freq ,fill=&quot;#00BFC4&quot;)) +
geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
xlab(&quot;&quot;) +
ylab(&quot;&quot;) +
theme_bw() +
coord_flip()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>El término <em>millones de dólares</em> aparece como una de las más mencionadas (122 veces) por el presidente, seguramente vinculada al contexto en que se realizan las declaraciones analizadas. A partir de esto, podríamos indagar un poco más el contexto de aparición de dicha expresión con la función <em>kwic</em> ( <em>keyword in context</em>) del paquete <a href="http://quanteda.io/">quanteda</a>. Esta técnica nos permite focalizar en una palabra o frase y analizar el contexto en que se menciona en un determinado texto, definiendo una <em>ventana</em> (cantidad de palabras anteriores y posteriores), pudiendo también constituir un nuevo corpus de datos textuales procesable. Podríamos analizar, por ejemplo, qué menciona el presidente en torno a palabras como <em>“sindicato”</em>, <em>“reforma de la seguridad social”</em> o <em>“transformación educativa”</em>.</p>
<pre class="r"><code>library(quanteda)

kwic=quanteda::kwic(declara_final$declara,phrase(&quot;millones de dólares&quot;),window = 15)%&gt;% as.data.frame()</code></pre>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-15">Table 3: </span>Contexto de palabra clave
</caption>
<thead>
<tr>
<th style="text-align:right;">
from
</th>
<th style="text-align:right;">
to
</th>
<th style="text-align:left;">
pre
</th>
<th style="text-align:left;">
keyword
</th>
<th style="text-align:left;">
post
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
755
</td>
<td style="text-align:right;">
757
</td>
<td style="text-align:left;">
que invertir gastar y o renuncia renunciar fiscalmente el último número que tengo son 1700
</td>
<td style="text-align:left;">
millones de dólares
</td>
<td style="text-align:left;">
pero creo que está más cerca de los dos mil millones de dólares por la
</td>
</tr>
<tr>
<td style="text-align:right;">
768
</td>
<td style="text-align:right;">
770
</td>
<td style="text-align:left;">
son 1700 millones de dólares pero creo que está más cerca de los dos mil
</td>
<td style="text-align:left;">
millones de dólares
</td>
<td style="text-align:left;">
por la consecuencia de la pandemia para ir resumiendo se logró equilibrar la economía se
</td>
</tr>
<tr>
<td style="text-align:right;">
1062
</td>
<td style="text-align:right;">
1064
</td>
<td style="text-align:left;">
recordación para hacer memoria cuando se creó el irpf decía que iba a recaudar 400
</td>
<td style="text-align:left;">
millones de dólares
</td>
<td style="text-align:left;">
Hoy está recaudando cerca de 2000 háblame háblame de voracidad fiscal nosotros preferimos más dinero
</td>
</tr>
<tr>
<td style="text-align:right;">
586
</td>
<td style="text-align:right;">
588
</td>
<td style="text-align:left;">
en el aeropuerto este año vamos a tener el récord histórico de obra pública 905
</td>
<td style="text-align:left;">
millones de dólares
</td>
<td style="text-align:left;">
aproximadamente ustedes lo verán en toda la ruta del país aquellas aún las menos transitadas
</td>
</tr>
<tr>
<td style="text-align:right;">
675
</td>
<td style="text-align:right;">
677
</td>
<td style="text-align:left;">
si le preguntaron las cifras son cifras complejas Cada día que no llueve son varios
</td>
<td style="text-align:left;">
millones de dólares
</td>
<td style="text-align:left;">
que el Uruguay está perdiendo Lamentablemente respecto al mensaje del 2 de marzo presidente hay
</td>
</tr>
</tbody>
</table>
<p>Otro ejercicio que podría hacer tiene que ver con buscar la correlación, con la función <em>textstat_simil</em> de <em>quanteda</em>, de una palabras con otras en términos de co-ocurrencia en cada una de las declaraciones públicas de Lacalle Pou. Como se observa, la palabra <em>libertad</em> en sus 144 menciones se asocia principalmente a nivel discursivo con la religión, las creencias, así como con otros términos que la complementan (suprema, ejercicio, demostración).</p>
<pre class="r"><code>library(quanteda)

quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
remove_numbers = T),tolower=TRUE,verbose = FALSE) %&gt;%
  quanteda::dfm_remove(pattern = c(quanteda::stopwords(&quot;spanish&quot;)),
min_nchar=3)%&gt;%
quanteda.textstats::textstat_simil(selection = &quot;libertad&quot;,
method = &quot;correlation&quot;,margin = &quot;features&quot;)%&gt;%
as.data.frame()%&gt;%
arrange(-correlation)%&gt;%
head(15)</code></pre>
<table style="width:90%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-17">Table 4: </span>Correlación entre palabras
</caption>
<thead>
<tr>
<th style="text-align:left;">
feature1
</th>
<th style="text-align:left;">
feature2
</th>
<th style="text-align:right;">
correlation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
demostraron
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5253191
</td>
</tr>
<tr>
<td style="text-align:left;">
religiosas
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5244921
</td>
</tr>
<tr>
<td style="text-align:left;">
aseguran
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5244921
</td>
</tr>
<tr>
<td style="text-align:left;">
creencias
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5244921
</td>
</tr>
<tr>
<td style="text-align:left;">
apelar
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5192013
</td>
</tr>
<tr>
<td style="text-align:left;">
suprema
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5141596
</td>
</tr>
<tr>
<td style="text-align:left;">
país
</td>
<td style="text-align:left;">
libertad
</td>
<td style="text-align:right;">
0.5117357
</td>
</tr>
</tbody>
</table>
<p>Por último, quiero ver como se distribuyen y agrupan las palabras de interés (sustantivos y nombres propios) en un espacio bidimensional en función de la distancia definida a partir de la co-ocurrencia en las declaraciones. Para esto, selecciono las 80 palabras más mencionadas y realizo un escalamiento multidimensional para definir las posiciones de cada una en el espacio. Luego, armo 8 clusters o agrupamientos de palabras que me van a dar información sobre los principales temas de los que habló Lacalle Pou, y que se desprenden de las palabras que se incluyen en cada uno. Algunos de los temas que tienen más cantidad de menciones están la pandemia ( <em>medidas</em>, <em>pandemia</em>), las vacunas ( <em>vacunas</em>, <em>dosis</em>, <em>vacunación</em>), reforma de la seguridad social ( <em>reforma</em>, <em>millones</em>, <em>coalición</em>).</p>
<pre class="r"><code>library(ggpubr)
vector=anotado %&gt;%
  filter(upos %in% c(&quot;PROPN&quot;,&quot;NOUN&quot;))%&gt;% pull(token)
dfm=quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
remove_numbers = T),tolower=TRUE,verbose = FALSE) %&gt;%
quanteda::dfm_remove(pattern = c(quanteda::stopwords(&quot;spanish&quot;)),
min_nchar=3)%&gt;% quanteda::dfm_trim(min_termfreq = 8)
base_fcm= dfm%&gt;%fcm(context = &quot;document&quot;)
base_fcm_select &lt;- fcm_select(base_fcm, pattern = vector,
selection = &quot;keep&quot;)
base_fcm_select &lt;- fcm_select(base_fcm, pattern = names(topfeatures(base_fcm, 80)),
selection = &quot;keep&quot;)
distancia &lt;- as.matrix(dist(base_fcm_select, method=&quot;canberra&quot;)) #canberra - método de distancia
mds &lt;- cmdscale(distancia,eig=TRUE, k=2) # 2 dimensiones
x &lt;- mds$points[,1]
y &lt;- mds$points[,2]
escalamiento &lt;- as.data.frame(mds$points) 
clusters &lt;- kmeans(escalamiento, 8)
clusters &lt;- as.factor(clusters$cluster)
escalamiento$Clusters =clusters
#Grafico
ggscatter(escalamiento,x = &quot;V1&quot;,y = &quot;V2&quot;,label = rownames(base_fcm_select),color = &quot;Clusters&quot;,palette = &quot;aaas&quot;,size = 1,ellipse = TRUE, ellipse.type = &quot;convex&quot;, 
repel = TRUE,font.label = c(7, &quot;plain&quot;),ggtheme = theme_bw())</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<hr />
<p>Hasta aquí algunos de los ensayos de recuperación y análisis de los datos textuales. Las técnicas que explora el posteo resultan útiles para realizar un análisis inicial y rápido de los principales tópicos, términos y sus combinaciones, agrupamientos de palabras, con visualizaciones ilustrativas que ayudan a interpretar los resultados. Pueden complementarse o combinarse con un análisis e interpretación en profundidad de los datos textuales, por ejemplo, de forma asistida con la herramienta libre de análisis cualitativo <a href="https://rqda.r-forge.r-project.org/">RQDA</a>, lo cual puede retroalimentar el análisis, a partir de identificar categorías teóricas de forma sistemática que se integren y oficien de variables de corte para las técnicas y procesamientos presentados.</p>
<hr />
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Para el año 2020 se recuperaron sólo 5 casos por lo que, cuando existan cruces por año, opto presentarlos en conjunto con los de 2021. Esto puede deberse a que se registró una baja en las declaraciones en situación de pandemia, existiendo una comunicación más centralizada, y en algunos casos, no aparecía la palabra <em>declaraciones</em> por lo que el filtro sub registró esos casos al automatizar la búsqueda.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
