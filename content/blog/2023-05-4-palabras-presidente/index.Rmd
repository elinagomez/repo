---
aliases: [palabras del presidente]
title: 'Las palabras del presidente'
thumbnail: ""
authors: [elina]
date: '2023-04-30'
tags: [Ciencias sociales computacionales]
codefolding_show: hide
codefolding_nobutton: true
categories:
  - R
  - Minería de texto
summary: 2023-05-04 / Recuperación y análisis textual de declaraciones presidenciales (2020-2023)
image:
  caption: ''
  focal_point: ''
  output:
  blogdown::html_page:
    toc: yes
    number_sections: true
    toc_depth: 4
featuredImage: ""
---


______________


Las palabras son un campo de disputa, están situadas, se conectan con un pasado y construyen realidad. En esta entrada me interesa ensayar algunas prácticas de recuperación de discursos políticos, en particular, declaraciones del presidente de la República Lacalle Pou y posterior análisis con técnicas de minería de texto usando R.   

Intentaremos responder lo siguiente: ¿Cuales son las palabras que son más mencionadas por el presidente en sus declaraciones? ¿y las frases? ¿esto cambia en el tiempo? ¿qué temas se desprenden del análisis de las palabras? ¿cómo se asocian entre sí?


______________

<br>

El ensayo se estructura en dos partes fundamentales: (1) recuperación y (2) procesamiento con visualizaciones asociadas. Para asegurar la replicabilidad del análisis se presenta el código utilizado (desplegable) y se disponibilizan en mi [repositorio de GitHub](https://github.com/elinagomez/repo/tree/master/content/blog/2023-05-4-palabras-presidente/Datos) las bases utilizadas para reproducir o ampliar los procesamientos.  


______________


**1. Recuperación:** scraping de declaraciones del presidente en YouTube


El primer desafío para cumplir con los objetivos que me había planteado es recuperar las declaraciones de Lacalle Pou en formato textual y procesable. Para ello, desde algún tiempo la plataforma YouTube permite descargar los subtítulos  de los videos para diferentes idiomas por lo que esa podía ser una vía para conseguir esos discursos o declaraciones. 

Para focalizar y construir un corpus de datos textuales exhaustivo de las declaraciones del presidente tenía algunas posibilidades. En primer lugar, lograr recopilar todas las url de las declaraciones haciendo una búsqueda simple en YouTube (por ej.: declaraciones+"lacalle pou") y copiándolas a una hoja de cálculo, pero me llevaría mucho tiempo por lo que opté por intentar automatizar la descargas de url con la librería [rvest](https://github.com/tidyverse/rvest) de tidyverse en múltiples páginas de resultado (tal como hice y está documentado en [este](https://www.elinagomez.com/blog/2020-09-26-parlamento-genero/) posteo de scraping parlamentario). Esta solución, aunque viable, no me permitía recuperar fácilmente alguna metadata que me interesaba para el análisis (fecha, resumen, canal, etc.) por lo que finalmente utilicé una herramienta de pago que se llama [Apify](https://apify.com/) que te da un crédito gratis suficiente para hacer varias búsquedas o scrapeos en diferentes plataformas de redes sociales (YouTube, Facebook, Instagram, Twitter) que puede ser interesante para usar como fuente en investigaciones sociales. 

Esta herramienta permite descargar las url (y alguna metadata) para luego poder ejecutar las transcripciones desde R, por lo que con una búsqueda específica (declaraciones+"lacalle pou") en 2 horas y 13 minutos me recuperó 575 resultados en formato tabulado que coincidían. Al analizar la base veo que es bastante precisa pero hay muchas declaraciones repetidas ya que existe una declaración oficial y los diferentes medios de comunicación la _"levantan"_ y suben en su propio canal de YouTube por lo que también existían intervenciones de periodistas que me _"ensuciaban"_ el análisis. A partir de esta constatación, opté por quedarme con las declaraciones oficiales desde la asunción del presidente (01/03/2020), subidas al canal oficial de YouTube de [Presidencia de la República](https://www.youtube.com/@ComunicacionPresidencial). Luego de hacer ese filtro inicial, me quedo con 205 enlaces y corroboro que todos los videos sean del presidente, a partir de lo cual descarto tres casos que eran de otras personas y no relevantes para mi análisis, me quedo con 202 url a videos. 

Para realizar la descarga de los subtitulos de los videos uso la librería [youtubecaption](https://github.com/jooyoungseo/youtubecaption) que trabaja sobre la librería de Python [youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/) y que que recupera la transcripción de forma tabulada y ordenada para cada secuencia del video, por lo que luego es necesario agrupar por el identificador y recuperar la metadata original. Hago este proceso iterando sobre mi vector de url con [purrr](https://purrr.tidyverse.org/) y la función _possibly()_ para saltar errores. 


```{r eval=F, message=FALSE, warning=FALSE}

remotes::install_github("jooyoungseo/youtubecaption") #instalo
library(youtubecaption) #cargo librerías
library(dplyr)

x <- base$url ##vector de url
declara <- purrr::map(x,purrr::possibly(~get_caption(.x,language = "es"),
otherwise = NULL))%>%
bind_rows() #itero, agrupo y uno lista


```



```{r echo=FALSE, message=FALSE, warning=FALSE}

load("C:/Users/Usuario/Documents/repo/content/blog/2023-05-4-palabras-presidente/Datos/base.RData")
library(youtubecaption)
library(dplyr)

x <- base$url[3]
declara <- purrr::map(x,purrr::possibly(~get_caption(.x,language = "es"),
           otherwise = NULL))%>%
           bind_rows()

knitr::kable(head(declara,6), table.attr = "style='width:90%;'",
                       caption = "Muestra de transcripciones de videos de YouTube con youtubecaption") %>% 
  kableExtra::kable_styling(position = "center")

```

<br>

Luego, agrupo por identificador y pego todas las transcripciones de cada secuencia de los videos, y recupero la metadata inicial para armar mi base final. Cabe aclarar que hay 17 casos que no se recuperaron los subtítulos porque eran imágenes del presidente sin audio.


```{r eval=F, message=FALSE, warning=FALSE}

declara_final = declara %>% 
  group_by(vid) %>% 
  mutate(declara = paste0(text, collapse = " ")) %>% #pego declaraciones por identificador
  distinct(vid,declara)%>%
  left_join(base,by=c("vid"="id"))%>% ##le pego la metadata
  mutate(anio=substr(date,1,4)) #armo variable de año

```


______________


**2. Análisis de los datos textuales** 


Como punto de partida para el análisis, luego de hacer la recuperación del apartado anterior, cuento con un corpus de datos con 185 declaraciones presidenciales transcriptas que comprenden el período 2020 a 2023[^1]. 

[^1]: Para el año 2020 se recuperaron sólo 5 casos por lo que, cuando existan cruces por año, opto presentarlos en conjunto con los de 2021. Esto puede deberse a que se registró una baja en las declaraciones en situación de pandemia, existiendo una comunicación más centralizada, y en algunos casos, no aparecía la palabra _declaraciones_ por lo que el filtro sub registró esos casos al automatizar la búsqueda. 

  
```{r echo=FALSE, message=FALSE, warning=FALSE}

load("C:/Users/Usuario/Documents/repo/content/blog/2023-05-4-palabras-presidente/Datos/declara_final.RData")
library(dplyr)
```

En términos de visualizaciones de los videos en YouTube, considerando todo el universo, se desprende que los tres de mayor relevancia fueron las declaraciones en el marco de la cumbre de la Celac (2023) y la Cumbre de Presidentes del Mercosur (2022), que tienen un carácter internacional y por lo tanto mayor impacto, y en tercer lugar, declaraciones vinculadas al caso de corrupción de Astesiano en noviembre de 2022.    


```{r echo=FALSE, message=FALSE, warning=FALSE}

load("C:/Users/Usuario/Documents/repo/content/blog/2023-05-4-palabras-presidente/Datos/declara_final.RData")
library(dplyr)

a=declara_final%>%
  select(title, text, date,viewCount)%>%
  arrange(-viewCount)%>%
  head(3)%>%
  select(-vid)


knitr::kable(a, table.attr = "style='width:90%;'",
                       caption = "Frecuencia de declaraciones recuperadas por año") %>% 
  kableExtra::kable_styling(position = "center",full_width = TRUE)


```


Me interesa explorar, como primer acercamiento, cuáles son las palabras que menciona el presidente en mayor medida pero quiero poder analizarlas distinguiendo entre sus categorías gramaticales. Para eso, necesito alguna herramienta que haga _anotaciones_ e identifique qué categoría corresponde a cada palabra que forma parte de mi corpus, y para ello voy a utilizar un modelo pre-entrenado para español que logra hacer ese etiquetado. Hay algunos paquetes de R para dicho fin (ej. [spacyr](https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html)) en base a la librería de Python _spacy_, pero en este caso vamos a usar [udpipe](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html) que definirá qué etiqueta (POS-tags) corresponde a cada palabra según [categorías universales](https://universaldependencies.org/u/pos/).



Si distinguimos entre categorías, como se ve en el gráfico, se observa que los adjetivos que predominan son de carácter positivo ( _importante_, _bueno_, _mejor_, etc.), mientras que los países que más se mencionan son _brasil_, _argentina_ y _paraguay_ respectivamente y el verbo más mencionado es _tener_. Aunque no entra en las categorías gramaticales que estamos viendo ya que es un adverbio, la palabra _obviamente_ es una muletilla del presidente, la menciona 506 veces, es decir, más de 2.5 veces por declaración.    


```{r eval=F, message=FALSE, warning=FALSE}

install.packages("udpipe")
library(udpipe)
#Descargo el modelo en una ruta determinada
modeloES <- udpipe_download_model(language = "ruta/spanish-gsd")
#Cargo el modelo desde esa ruta
modeloES <- udpipe_load_model(language = "spanish-gsd")
anotado <- udpipe_annotate(modeloES, x = declara_final$declara,
doc_id = declara_final$vid)%>%
as.data.frame()

##Gráfico según categoría gramatical

anotado %>%
  filter(upos %in% c("NOUN","PROPN","ADJ","VERB"))%>%
  mutate(upos = recode(upos, NOUN = 'Sustantivos', PROPN = 'Nombres propios', 
  ADJ =  'Adjetivos',VERB =  'Verbos' ))%>%
  filter(upos%in%quanteda::stopwords(language = "es")==F)%>%
  group_by(lemma,upos) %>%
  summarize(n=n())%>%
  group_by(upos)%>%
  top_n(15, abs(n)) %>%
  ggplot(aes(reorder(lemma, n), n,fill=upos)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ upos, scales = "free") +
  xlab("") +
  ylab("") +
  theme_bw() +
  coord_flip()


```

```{r message=FALSE, warning=FALSE, include=FALSE}


load("C:/Users/Usuario/Documents/repo/content/blog/2023-05-4-palabras-presidente/Datos/anotado.RData")
library(ggplot2)

anotado %>%
  filter(upos %in% c("NOUN","PROPN","ADJ","VERB"))%>%
  mutate(upos = recode(upos, NOUN = 'Sustantivos', PROPN = 'Nombres propios', 
  ADJ =  'Adjetivos', VERB =  'Verbos' ))%>%
  filter(upos%in%quanteda::stopwords(language = "es")==F)%>%
  group_by(lemma,upos) %>%
  summarize(n=n())%>%
  group_by(upos)%>%
  top_n(15, abs(n)) %>%
  ggplot(aes(reorder(lemma, n), n,fill=upos)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ upos, scales = "free") +
  xlab("") +
  ylab("") +
  theme_bw() +
  coord_flip()

```

<iframe src="unnamed-chunk-7-1.png" width="700"  height="500"></iframe>

<br>

Luego, voy a focalizar el análisis en algunas palabras específicas y observar la evolución entre los años de los cuales tenemos declaraciones, en un mapa de calor. Para ello, calculo el peso relativo de cada palabra en el total de palabras mencionadas por el presidente ese año (ya que tenemos menos declaraciones de 2023) y comparo. Tiene sentido que en los primeros años (2020 y 2021) tenga más peso las palabras _pandemia_ y _salud_, mientras que social y economía empiezan a tener más relevancia desde 2022. Las palabras corrupción, sindicato e ideología tienen un peso relativo menor con respecto a las demás.  


```{r eval=FALSE, message=FALSE, warning=FALSE}

anotado %>%
  left_join(.,declara_final%>%select(anio),by=c("doc_id"="vid"))%>%
  filter(upos%in%quanteda::stopwords(language = "es")==F)%>%
  group_by(anio,lemma) %>%
  summarise(n = n()) %>%
  mutate(prop = round((n / sum(n)*100),3))%>%
  filter(lemma %in% c("libertad","ideología","pandemia","economía","social","salud",
                      "coalición","corrupción","argentina","brasil","china","reforma",
                      "sindicato"))%>% ##palabras que selecciono
group_by(anio)%>%
ggplot(aes(x = anio , y = reorder(lemma, prop))) +
  geom_tile(aes(fill = prop), color = "white", size = 1) +
  scale_fill_gradient(high = "#C77CFF",low = "white") +
  theme_bw() +
  theme(axis.ticks = element_blank(),
        panel.background = element_blank(),
        axis.title = element_blank())

```


```{r message=FALSE, warning=FALSE, include=FALSE}

anotado %>%
  left_join(.,declara_final%>%select(anio),by=c("doc_id"="vid"))%>%
  filter(upos%in%quanteda::stopwords(language = "es")==F)%>%
  group_by(anio,lemma) %>%
  summarise(n = n()) %>%
  mutate(prop = round((n / sum(n)*100),3))%>%
  filter(lemma %in% c("libertad","ideología","pandemia","economía","social","salud",
                      "coalición","corrupción","argentina","brasil","china","reforma",
                      "sindicato"))%>%
  group_by(anio)%>%
   #top_n(15, abs(prop)) %>%
ggplot(aes(x = anio , y = reorder(lemma, prop)      )) +
  geom_tile(aes(fill = prop), color = "white", size = 1) +
  scale_fill_gradient(high = "#C77CFF",low = "white") +
  theme_bw() +
  theme(axis.ticks = element_blank(),
        panel.background = element_blank(),
        axis.title = element_blank())

```

<iframe src="unnamed-chunk-9-1.png" width="700"  height="500"></iframe>


<br>

Si quiero ir un poco más allá de las palabras y extraer los términos claves que menciona Lacalle Pou en sus declaraciones, me quedo únicamente con los sustantivos, adjetivos y nombres propios, y utilizo la función _keywords_rake_ del mismo paquete [udpipe](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html) que construye una métrica ( _ranke_) que surge de combinar la frecuencia de aparición de una palabra y la co-ocurrencia con otras en un documento determinado, en este caso, en cada declaración. Como criterio general, voy a considerar términos claves de hasta tres palabras y opto por quedarme con el _lema_ de cada una, el cual corresponde a la forma común (ej. público) a todas las formas que puede adquirir una palabra (públicos, pública, pública, etc.) y así evitar repeticiones.     


Las palabras claves nos dan una idea de las temáticas que trata el presidente en sus declaraciones públicas y su jerarquización en términos de presencia mayoritaria de ciertos términos. Lo sanitario aparece en primer lugar, vinculado a la pandemia, seguido por la ley de urgente consideración, así como tópicos sobre impuestos, aranceles y comercio exterior.  


```{r eval=FALSE, message=FALSE, warning=FALSE}

udpipe::keywords_rake(x = anotado %>%filter(lemma %in%quanteda::stopwords(language = "es")==F),term = "lemma", group = "doc_id", 
relevant = anotado %>%filter(lemma %in%quanteda::stopwords(language = "es")==F)%>% pull(upos) %in% c("NOUN", "ADJ","PROPN"),
ngram_max = 3)%>% # Sólo sustantivos, adjetivos y nombres propios
filter(freq>4)%>%
  arrange(rake) %>%
  top_n(30, abs(rake)) %>%
  ggplot(aes(reorder(keyword, rake), rake )) +
  geom_bar(stat = "identity", show.legend = FALSE,fill= "#00bfc4") +
   scale_y_continuous(limits = c(0,NA)) +
  xlab("") +
  ylab("") +
  theme_bw() +
  coord_flip()

```


```{r message=FALSE, warning=FALSE, include=FALSE}

udpipe::keywords_rake(x = anotado %>%filter(lemma %in%quanteda::stopwords(language = "es")==F),term = "lemma", group = "doc_id", 
relevant = anotado %>%filter(lemma %in%quanteda::stopwords(language = "es")==F)%>% pull(upos) %in% c("NOUN", "ADJ","PROPN"),
ngram_max = 3)%>% 
filter(freq>4)%>%
  arrange(rake) %>%
  top_n(30, abs(rake)) %>%
  ggplot(aes(reorder(keyword, rake), rake )) +
  geom_bar(stat = "identity", show.legend = FALSE,fill= "#00bfc4") +
   scale_y_continuous(limits = c(0,NA)) +
  xlab("") +
  ylab("") +
  theme_bw() +
  coord_flip()

```

<iframe src="unnamed-chunk-11-1.png" width="700"  height="500"></iframe>

<br>


Otra forma de acercarnos a lo que se menciona en las declaraciones, tiene que ver con la identificación de _frases_ o _sintagmas nominales_, es decir, que se construyen a partir de un _sustantivo_ como núcleo de la frase. Esto lo puedo identificar utilizando [expresiones regulares](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular) que me permiten extraer frases a partir de un patrón que combina las formas gramaticales previamente etiquetadas. Con ese fin, uso la función _keywords_phrases_. 


```{r eval=FALSE, message=FALSE, warning=FALSE}
library(udpipe)
anotado$phrase_tag <- udpipe::as_phrasemachine(anotado$upos, type = "upos")
anotado$phrase_tag[anotado$upos=="PRON"] <- "O"
anotado$phrase_tag[anotado$upos=="NUM"] <- "O"
udpipe::keywords_phrases(x = anotado$phrase_tag, term = tolower(anotado$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)%>%
filter(ngram > 1 & freq > 3)%>%
arrange(freq) %>%
top_n(30, abs(freq)) %>%
ggplot(aes(reorder(keyword, freq), freq ,fill="#00BFC4")) +
geom_bar(stat = "identity", show.legend = FALSE) +
xlab("") +
ylab("") +
theme_bw() +
coord_flip()

```


```{r message=FALSE, warning=FALSE, include=FALSE}
library(udpipe)
anotado$phrase_tag <- udpipe::as_phrasemachine(anotado$upos, type = "upos")
anotado$phrase_tag[anotado$upos=="PRON"] <- "O"
anotado$phrase_tag[anotado$upos=="NUM"] <- "O"
udpipe::keywords_phrases(x = anotado$phrase_tag, term = tolower(anotado$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)%>%
filter(ngram > 1 & freq > 3)%>%
arrange(freq) %>%
top_n(30, abs(freq)) %>%
ggplot(aes(reorder(keyword, freq), freq ,fill="#00BFC4")) +
geom_bar(stat = "identity", show.legend = FALSE) +
xlab("") +
ylab("") +
theme_bw() +
coord_flip()

```

<iframe src="unnamed-chunk-13-1.png" width="700"  height="500"></iframe>

<br>

El término _millones de dólares_ aparece como una de las más mencionadas (122 veces) por el presidente, seguramente vinculada al contexto en que se realizan las declaraciones analizadas. A partir de esto, podríamos indagar un poco más el contexto de aparición de dicha expresión con la función _kwic_ ( _keyword in context_) del paquete [quanteda](http://quanteda.io/). Esta técnica nos permite focalizar en una palabra o frase y analizar el contexto en que se menciona en un determinado texto, definiendo una _ventana_ (cantidad de palabras anteriores y posteriores), pudiendo también constituir un nuevo corpus de datos textuales procesable. Podríamos analizar, por ejemplo, qué menciona el presidente en torno a palabras como _"sindicato"_, _"reforma de la seguridad social"_ o _"transformación educativa"_. 


```{r eval=FALSE, message=FALSE, warning=FALSE}

library(quanteda)

kwic=quanteda::kwic(declara_final$declara,phrase("millones de dólares"),window = 15)%>% as.data.frame()

```


```{r echo=FALSE, message=FALSE, warning=FALSE}

library(quanteda)
library(tidyr)

kwic=quanteda::kwic(declara_final$declara,phrase("millones de dólares"),window = 15)%>% as.data.frame()%>%select(from,to,pre,keyword,post)%>% head(5)

Encoding(kwic$pre)="UTF-8"
Encoding(kwic$post)="UTF-8"

knitr::kable(kwic, table.attr = "style='width:90%;'",
caption = "Contexto de palabra clave") %>% 
kableExtra::kable_styling(position = "center")

```


Otro ejercicio que podría hacer tiene que ver con buscar la correlación, con la función _textstat_simil_ de _quanteda_, de una palabras con otras en términos de co-ocurrencia en cada una de las declaraciones públicas de Lacalle Pou. Como se observa, la palabra _libertad_ en sus 144 menciones se asocia principalmente a nivel discursivo con la religión, las creencias, así como con otros términos que la complementan (suprema, ejercicio, demostración). 


```{r eval=FALSE, message=FALSE, warning=FALSE}
library(quanteda)

quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
remove_numbers = T),tolower=TRUE,verbose = FALSE) %>%
  quanteda::dfm_remove(pattern = c(quanteda::stopwords("spanish")),
min_nchar=3)%>%
quanteda.textstats::textstat_simil(selection = "libertad",
method = "correlation",margin = "features")%>%
as.data.frame()%>%
arrange(-correlation)%>%
head(15)

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(quanteda)

dfm=quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
remove_numbers = T),tolower=TRUE,verbose = FALSE) %>%
  quanteda::dfm_remove(pattern = c(quanteda::stopwords("spanish")),
min_nchar=3)
a=quanteda.textstats::textstat_simil(dfm,selection = "libertad",
method = "correlation",margin = "features")%>%
as.data.frame()%>%
  arrange(-correlation)%>%
head(7)

knitr::kable(a, table.attr = "style='width:90%;'",
caption = "Correlación entre palabras") %>% 
kableExtra::kable_styling(position = "center")


```


<br>
Por último, quiero ver como se distribuyen y agrupan las palabras de interés (sustantivos y nombres propios) en un espacio bidimensional en función de la distancia definida a partir de la co-ocurrencia en las declaraciones. Para esto, selecciono las 80 palabras más mencionadas y realizo un escalamiento multidimensional para definir las posiciones de cada una en el espacio. Luego, armo 8 clusters o agrupamientos de palabras que me van a dar información sobre los principales temas de los que habló Lacalle Pou, y que se desprenden de las palabras que se incluyen en cada uno. Algunos de los temas que tienen más cantidad de menciones están la pandemia ( _medidas_, _pandemia_), las vacunas ( _vacunas_, _dosis_, _vacunación_), reforma de la seguridad social ( _reforma_, _millones_, _coalición_). 


```{r eval=FALSE, message=FALSE, warning=FALSE}

library(ggpubr)
vector=anotado %>%
  filter(upos %in% c("PROPN","NOUN"))%>% pull(token)
dfm=quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
remove_numbers = T),tolower=TRUE,verbose = FALSE) %>%
quanteda::dfm_remove(pattern = c(quanteda::stopwords("spanish")),
min_nchar=3)%>% quanteda::dfm_trim(min_termfreq = 8)
base_fcm= dfm%>%fcm(context = "document")
base_fcm_select <- fcm_select(base_fcm, pattern = vector,
selection = "keep")
base_fcm_select <- fcm_select(base_fcm, pattern = names(topfeatures(base_fcm, 80)),
selection = "keep")
distancia <- as.matrix(dist(base_fcm_select, method="canberra")) #canberra - método de distancia
mds <- cmdscale(distancia,eig=TRUE, k=2) # 2 dimensiones
x <- mds$points[,1]
y <- mds$points[,2]
escalamiento <- as.data.frame(mds$points) 
clusters <- kmeans(escalamiento, 8)
clusters <- as.factor(clusters$cluster)
escalamiento$Clusters =clusters
#Grafico
ggscatter(escalamiento,x = "V1",y = "V2",label = rownames(base_fcm_select),color = "Clusters",palette = "aaas",size = 1,ellipse = TRUE, ellipse.type = "convex", 
repel = TRUE,font.label = c(7, "plain"),ggtheme = theme_bw())


```

```{r message=FALSE, warning=FALSE, include=FALSE}

library(ggpubr)
vector=anotado %>%
  filter(upos %in% c("PROPN","NOUN"))%>% pull(token)

dfm=quanteda::dfm(tokens(declara_final$declara,remove_punct = T,
remove_numbers = T),tolower=TRUE,verbose = FALSE) %>%
  quanteda::dfm_remove(pattern = c(quanteda::stopwords("spanish"),"aplausos"),
  min_nchar=3)%>%
  quanteda::dfm_trim(min_termfreq = 8)
base_fcm= dfm%>%
  fcm(context = "document")
base_fcm_select <- fcm_select(base_fcm, pattern = vector,
selection = "keep")
base_fcm_select <- fcm_select(base_fcm, pattern = names(topfeatures(base_fcm, 80)),
selection = "keep")
distancia <- as.matrix(dist(base_fcm_select, method="canberra"))
mds <- cmdscale(distancia,eig=TRUE, k=2)
x <- mds$points[,1]
y <- mds$points[,2]
escalamiento <- as.data.frame(mds$points) 
clusters <- kmeans(escalamiento, 8)
clusters <- as.factor(clusters$cluster)
escalamiento$Clusters =clusters


ggscatter(escalamiento,x = "V1",y = "V2", 
          label = rownames(base_fcm_select), 
          color = "Clusters", 
          palette = "aaas", 
          size = 1, 
          ellipse = TRUE, 
          ellipse.type = "convex", 
          repel = TRUE,
          font.label = c(7, "plain"),
          ggtheme = theme_bw())


```

<iframe src="unnamed-chunk-19-1.png" width="700"  height="500"></iframe>

______________


Hasta aquí algunos de los ensayos de recuperación y análisis de los datos textuales. Las técnicas que explora el posteo resultan útiles para realizar un análisis inicial y rápido de los principales tópicos, términos y sus combinaciones, agrupamientos de palabras, con visualizaciones ilustrativas que ayudan a interpretar los resultados. Pueden complementarse o combinarse con un análisis e interpretación en profundidad de los datos textuales, por ejemplo, de forma asistida con la herramienta libre de análisis cualitativo [RQDA](https://rqda.r-forge.r-project.org/), lo cual puede retroalimentar el análisis, a partir de identificar categorías teóricas de forma sistemática que se integren y oficien de variables de corte para las técnicas y procesamientos presentados.    

______________

