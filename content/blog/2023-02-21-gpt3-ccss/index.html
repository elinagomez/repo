---
aliases: [GPT-3 ciencias sociales]
title: 'Uso de GPT-3 en las ciencias sociales: algunas ideas'
thumbnail: ""
authors: [elina]
date: '2023-02-21'
tags: [Ciencias sociales computacionales]
categories:
  - R
  - Minería de texto
  - GPT-3
summary: 2023-02-21 / Uso de GPT-3 en las ciencias sociales
image:
  caption: ''
  focal_point: ''
  output:
  blogdown::html_page:
    toc: false
    number_sections: true
    toc_depth: 1
featuredImage: ""
---

<script src="index_files/header-attrs/header-attrs.js"></script>


<p>En esta entrada me interesaba indagar acerca de las potencialidades que podría tener GPT-3, desde el punto de vista metodológico, para las investigaciones que hacemos en ciencias sociales, pero…empezando por el principio <strong>¿qué es GPT-3?</strong></p>
<p>Bueno, <strong>GPT-3</strong> es un modelo de lenguaje basado en Inteligencia Artificial (IA), desarrollado por la empresa <a href="https://openai.com/">OpenAI</a>, que ha tenido gran repercusión en los últimos meses y que tiene capacidad de predecir palabras o frases dado un contexto determinado. El texto que genera es posible a través de un pre-entrenamiento de modelo con un gran volumen de datos textuales de lenguaje natural. El acceso gratuito tiene un tope (U$S 18 para usar en 3 meses) y podemos conectarlo con RStudio.</p>
<hr />
<p>Presento dos posibles aplicaciones para optimizar nuestros procesos:</p>
<ol style="list-style-type: decimal">
<li><p>Identificación de tópicos o temas (binarios) en textos medianos o largos.</p></li>
<li><p>Codificación de preguntas abiertas a partir de <em>codigueras pre-definidas</em> o <em>emergentes</em></p></li>
</ol>
<hr />
<p>La librería que usé para el procesamiento es <a href="https://github.com/ben-aaron188/rgpt3">rgpt3</a> de Bennett Kleinberg (2022)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>El paquete tiene una función que se llama <strong>gpt3_authenticate()</strong> y que te permite, luego de crear un usuario <a href="https://openai.com/api/">acá</a>, acceder a una <a href="https://platform.openai.com/account/api-keys">clave</a> que va a permitirte conectar con la API de GPT-3. La clave debe ser guardada en un archivo de notas (.txt) y va a ser el primer argumento de la función para conectar. Luego, chequeo que la conexión se estableció con éxito con la función <strong>gpt3_test_completion()</strong></p>
<pre class="r"><code>#Cargo paquete ya instalado:
library(rgpt3)

gpt3_authenticate(&quot;ruta/access_key.txt&quot;) ##le indico dónde tengo guardado el archivo con la clave
gpt3_test_completion() ##testeo la conexión</code></pre>
<hr />
<p>Luego de estos pasos previos, <strong>ya tengo conectada</strong> mi consola R+RStudio con la API de GPT-3 !</p>
<p>Empecemos…</p>
<p><strong>1.</strong> Identificación de tópicos o temas (binarios) en textos medianos o largos.</p>
<p>Me interesa saber, más allá de una clasificación genérica de menciones parlamentarias que tocan la temática de género(como lo hice en una entrada anterior), saber cuales de ellas hablan específicamente del tema <em>violencia de género</em>, y para eso uso la asistencia del modelo GPT-3 para que me ayude a identificarlas. Según algunos ensayos previos, veo que lo más optimo es hacerlo en dos pasos: en primer lugar le pido que identifique un <em>Tema principal</em> en cada mención (lo cual ya me podría ser útil y analizable en sí mismo), ya que la intervención podría tratar varios temas y, en segundo lugar, le doy una orden concreta para que identifique si ese tema se vincula con mi tópico de interés y le sugiero devolverme un <strong>Si</strong> si le parece que lo trata y un <strong>No</strong>, en el caso contrario.</p>
<p><em>Nota:</em> Esto también podría servir para un análisis rápido de corpus de noticias o post de redes sociales. Más adelante vamos a ver que para textos cortos utilizo otra estrategia de clasificación/extracción de tópicos.</p>
<p>Las ordenes que le voy a dar son: <em>‘Identifique un tema principal en el siguiente texto:’</em> y <em>‘Este texto habla sobre violencia de género? Responda únicamente Si o No’</em> y luego, le pego cada una de las menciones y variables de origen de cada parlamentario/a, de este modo para el segundo caso:</p>
<pre class="r"><code>prompt = data.frame(&#39;prompts&#39; = c(paste(&#39;La siguiente frase menciona la violencia de género? Responda únicamente Si o No&#39;,base$speech)),&#39;prompt_id&#39; = c(1:nrow(base)))



consulta = gpt3_completions(prompt_var = prompt$prompts # defino las órdenes, una por cada mención
                             , id_var = prompt$prompt_id # el identificador
                             , param_model = &#39;text-davinci-003&#39;, ##defino el modelo
                            param_max_tokens = 2000,param_output_type = &quot;complete&quot;,
                            param_temperature = 0) ##defino algunos parámetros: max_tokens (cuanto me va a traer como máximo, debe ser mayor que mi N y temperature que está entre 0 y 1, siendo las respuestas que se acercan a 1 más aleatorias) 

respuestas=consulta[[1]] ##veo el primer elemento en mi lista que es mi data frame de respuestas
base=cbind(base,respuestas) ##le pego las respuestas a la base original</code></pre>
<!-- <iframe src="imagenes/t1.html" width="900"  height="500"></iframe> -->
<p><strong>2.</strong> Codificación de preguntas abiertas a partir de <em>codigueras pre-definidas</em> o <em>emergentes</em></p>
<p>Otra aplicación posible sería el tratamiento de preguntas abiertas usando el GTP-3 como un asistente para la codificación (lo cual nos podría ahorrar mucho tiempo!), al menos para una clasificación inicial cuando tenemos muchos casos.</p>
<p>Identifico dos formas: (a) <em>Codigueras pre-definidas</em> o (b) <em>Emergentes</em></p>
<p>En el primer caso, dentro de la orden que le damos y que vimos en el caso anterior, le defino las categorías que deseo que identifique en cada caso. Hay un paper reciente (Bailey et al, 2022)<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> que compara para un set de preguntas abiertas, un codificador humano, un modelo de aprendizaje automático (SVM) y el modelo GPT-3, y encuentra niveles de acierto altos(recomiendo la lectura! aunque sin dudas deben existir ventajas en el inglés).</p>
<p>El ensayo que hice fue de una encuesta de Opinión Pública, sobre la típica pregunta sobre los principales problemas del país, en este caso lo ideal es tener una respuesta única pero sin embargo, se puede definir en la orden (prompt) un criterio de priorización de algún tipo (ej. primera mención).</p>
<pre class="r"><code>prompt = data.frame(&#39;prompts&#39; = c(paste(&quot;Tengo algunas respuestas abiertas de una encuesta que pregunta lo siguiente: ¿qué cosas le preocupan en su vida?. 
Por favor, asigne una de las siguientes categorías a cada respuesta de texto abierto.
La categoría son: 

salud
economía
educación 
seguridad
política
pobreza
sueldos
desempleo
corrupción
suba de precios
espacios
transporte público
migración
vivienda
costo del estado

Si no es ninguna de las anteriores por favor asigne la categoría Otros&quot;,
base_op$R1)),&#39;prompt_id&#39; = c(1:nrow(base_op)))

consulta = gpt3_completions(prompt_var = my_prompts$prompts
                            , id_var = my_prompts$prompt_id
                            , param_model = &#39;text-davinci-003&#39;,
                            param_max_tokens = 500,param_temperature = 0,
                            param_output_type = &quot;complete&quot;)


respuestas=consulta[[1]]
base_op=cbind(base_op,respuestas)</code></pre>
<pre class="r"><code>library(dplyr)
library(DT)
load(&quot;Datos/base_op_codiguera.RData&quot;)

a=base_op_codiguera %&gt;%
  select(prompt,SIM1,gpt3)%&gt;%
  rename(&quot;respuesta&quot;=SIM1)

b=DT::datatable(a,rownames = TRUE, options = list(pageLength = 50,dom = &#39;Bfrtip&#39;,
scrollX = T,columnDefs = list(list(targets = c(1,2),render = DT::JS(
                                                                                          &quot;function(data, type, row, meta) {&quot;,
                                                                                                 &quot;return type === &#39;display&#39; &amp;&amp; data.length &gt; 40 ?&quot;,
                                                                                                 &quot;&#39;&lt;span title=\&quot;&#39; + data + &#39;\&quot;&gt;&#39; + data.substr(0, 40) + &#39;...&lt;/span&gt;&#39; : data;&quot;,&quot;}&quot;)))), callback = JS(&#39;table.page(3).draw(false);&#39;))


b=DT::datatable(a,rownames = TRUE, options = list(pageLength = 50,dom = &#39;Bfrtip&#39;,
scrollX = T,columnDefs = list(list(targets = c(1,2),render = DT::JS(
                                                                                          &quot;function(data, type, row, meta) {&quot;,
                                                                                                 &quot;return type === &#39;display&#39; &amp;&amp; data.length &gt; 40 ?&quot;,
                                                                                                 &quot;&#39;&lt;span title=\&quot;&#39; + data + &#39;\&quot;&gt;&#39; + data.substr(0, 40) + &#39;...&lt;/span&gt;&#39; : data;&quot;,&quot;}&quot;)))), callback = JS(&#39;table.page(3).draw(false);&#39;))

# htmlwidgets::saveWidget(frameableWidget(b), &quot;C:/Users/Usuario/Desktop/2023-02-21-gpt3-ccss/t2.html&quot;,selfcontained = TRUE)

# htmlwidgets::saveWidget(b, &quot;C:/Users/Usuario/Desktop/2023-02-21-gpt3-ccss/t2.html&quot;,selfcontained = TRUE)

frameWidget(b)</code></pre>
<!-- <iframe src="t2.html" width="900"  height="500"></iframe>  -->
<p>Por último, en el caso de preguntas que no tengan, o sea difícil, una codificación previa, voy a optar por una orden más concreta que me permita identificar códigos emergentes. En el ejemplo que utilicé, era una pregunta también de opinión pública orientada en conocer sobre qué genera bienestar de las personas consultadas. La orden diseñada fue:</p>
<pre class="r"><code>prompt = data.frame(&#39;prompts&#39; = c(paste(&quot;Tengo algunas respuestas abiertas de una encuesta que pregunta: ¿Qué cosas le causan bienestar, qué le hace feliz o le pone contento?. 
Por favor identifique en pocas palabras las principales respuestas&quot;,base_op$R2)),
&#39;prompt_id&#39; = c(1:nrow(base_op)))

consulta = gpt3_completions(prompt_var = prompt$prompts
                            , id_var = prompt$prompt_id
                            , param_model = &#39;text-davinci-003&#39;,param_max_tokens = 2000,
                            param_temperature = 0,
                            param_output_type = &quot;complete&quot;)



respuestas_2=consulta[[1]]
base_op=cbind(base_op,respuestas_2)</code></pre>
<pre class="r"><code>library(dplyr)
library(DT)
library(htmlwidgets)
library(plotly)
load(&quot;Datos/bienestar_muestra.RData&quot;)

a=base_op %&gt;%
  filter(id&lt;=9 &amp; id&gt;1)%&gt;%
  select(prompt,SIM2,gpt3)%&gt;%
  rename(&quot;respuesta&quot;=SIM2)


b=DT::datatable(a,rownames = TRUE, options = list(pageLength = 50,dom = &#39;Bfrtip&#39;,
scrollX = T,columnDefs = list(list(targets = c(1),render = DT::JS(
                                                                                          &quot;function(data, type, row, meta) {&quot;,
                                                                                                 &quot;return type === &#39;display&#39; &amp;&amp; data.length &gt; 40 ?&quot;,
                                                                                                 &quot;&#39;&lt;span title=\&quot;&#39; + data + &#39;\&quot;&gt;&#39; + data.substr(0, 40) + &#39;...&lt;/span&gt;&#39; : data;&quot;,&quot;}&quot;)))), callback = JS(&#39;table.page(3).draw(false);&#39;))


# htmlwidgets::saveWidget(b, &quot;C:/Users/Usuario/Documents/repo/content/blog/2023-02-21-gpt3-ccss/imagenes/t3.html&quot;,selfcontained = TRUE)</code></pre>
<!-- <iframe src="imagenes/t3.html" width="900"  height="500"></iframe> -->
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Kleinberg, B. (2022). rgpt3: Making requests from R to the GPT-3 API (Version 0.3.1) [Computer software]. <a href="https://doi.org/10.5281/zenodo.7327667" class="uri">https://doi.org/10.5281/zenodo.7327667</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Mellon, Jonathan and Bailey, Jack and Scott, Ralph and Breckwoldt, James and Miori, Marta, Does GPT-3 know what the Most Important Issue is? Using Large Language Models to Code Open-Text Social Survey Responses At Scale (December 22, 2022). Available at SSRN: <a href="https://ssrn.com/abstract=4310154" class="uri">https://ssrn.com/abstract=4310154</a> or <a href="http://dx.doi.org/10.2139/ssrn.4310154" class="uri">http://dx.doi.org/10.2139/ssrn.4310154</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
